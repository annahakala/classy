from pathlib import Path
import os
import random
import numpy as np

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

def load_files(load_dir):

    files = os.listdir(load_dir)

    for i in range(len(files)):
        files[i] = os.path.join(load_dir,files[i])
    return files

def create_dataset(files, val_data_amount=0.3, sequence_max_length=30, step=3):
    """ Creates a dataset and splits it into training and validation data.
    Returns the training and validation datasets as well as the enumerations.
    @TODO maybe save the datasets to a folder instead of returning them
    """
    dataset = []
    for file in files:
        f = open(file,'r')
        f = f.read().split()
        #f = ['start'] + f + ['stop'] #Add start and stop to mark when a song starts or ends

        dataset += f

    dataset += [SOS_token, EOS_token]
    notes = list(dict.fromkeys(dataset))
    print("Total amount of unique symbols: ", len(notes))

    notes_indices = dict((c, i) for i, c in enumerate(notes))
    indices_notes = dict((i, c) for i, c in enumerate(notes))

    for i in range(len(dataset)):
        dataset[i] = notes_indices[dataset[i]]

    sequences = []   
    next_notes = []

    SOS_ind = notes_indices[SOS_token]
    EOS_ind = notes_indices[EOS_token]

    for i in range(0, len(dataset) - sequence_max_length, step):
        sequences.append([SOS_ind] + dataset[i: i+sequence_max_length] + [EOS_ind])
        next_notes.append([SOS_ind, dataset[i + sequence_max_length], EOS_ind])

    print("Generated ", len(sequences), "sequences")

    val_samples = int(val_data_amount*len(sequences))

    #Split the dataset into training and validation data

    sequences = np.array(sequences)
    np.random.shuffle(sequences)

    t_seqs_input = sequences[:val_samples]
    t_seqs_target = next_notes[:val_samples]

    v_seqs_input = sequences[val_samples:]
    v_seqs_target = next_notes[val_samples:]
    
    print("Split the dataset into ",len(t_seqs_input), " training samples and ", len(v_seqs_input), " validation samples.")

    return t_seqs_input, t_seqs_target, v_seqs_input, v_seqs_target, notes_indices, indices_notes


class Encoder(nn.Module):
    def __init__(self, dictionary_size, hidden_size):
        """
        Args:
          dictionary_size (int): Size of dictionary in the source language.
          hidden_size (int): Size of the hidden state.
        """
        super(Encoder, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(dictionary_size, hidden_size)
        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size)

    def forward(self, input_seq, hidden):
        """
        Args:
          input_seq (tensor):  Tensor of words (word indices) of the input sentence. The shape is
                               [seq_length, batch_size] with batch_size = 1.
          hidden (tensor):    The state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).

        Returns:
          output (tensor): Output of the GRU (shape [seq_length, 1, hidden_size]).
          hidden (tensor): New state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).
        """
        #print("Input seq size is: ",input_seq.size())
        batch_size = input_seq.size(1)
        assert batch_size == 1, "Encoder can process only one sequence at a time."
        
        # YOUR CODE HERE
        embedded = self.embedding(input_seq)
        outputs, hidden = self.gru(embedded, hidden)

        return outputs, hidden

    def init_hidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

class Decoder(nn.Module):
    def __init__(self, hidden_size, output_dictionary_size):
        """
        Args:
          hidden_size (int): Size of the hidden state.
          output_dictionary_size (int): Size of dictionary in the target language.
        """
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size

        # YOUR CODE HERE
        self.embedding = nn.Embedding(output_dictionary_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_dictionary_size)
        self.softmax = nn.LogSoftmax(dim=2)

    def forward(self, hidden, target_seq=None, teacher_forcing=False):
        """
        Args:
          hidden (tensor):        The state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).
          target_seq (tensor):    Tensor of words (word indices) of the target sentence. The shape is
                                   [target_seq_length, batch_size] with batch_size=1. If None, the output sequence
                                   is generated by feeding the decoder's outputs (teacher_forcing has to be False).
          teacher_forcing (bool): Whether to use teacher forcing or not.

        Returns:
          outputs (tensor): Tensor of log-probabilities of words in the output language
                             (shape [output_seq_length, batch_size, output_dictionary_size] with batch_size=1).
          hidden (tensor):  New state of the GRU (shape [1, batch_size, hidden_size] with batch_size=1).
        """
        
        #print("Target seq is: " + str(target_seq) + ", Teacher forcing is: " + str(teacher_forcing))
        
        if target_seq is None:
            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'

        prev_word = torch.tensor([SOS_token], device=device, dtype=torch.int64)
        out_length = target_seq.size(0) if target_seq is not None else MAX_LENGTH
        outputs = []  # Collect decoder outputs at different processing steps in this list
        for t in range(out_length):
            # YOUR CODE HERE
            
            output = self.embedding(prev_word)
            output = output.view(1, 1, -1)
            output = F.relu(output)
            output, hidden = self.gru(output, hidden)
            
            output = self.out(output)
            output = self.softmax(output)
            
            outputs.append(output)
            
            if teacher_forcing:
                # Feed the target as the next input
                prev_word = target_seq[t]  # Teacher forcing
            else:
                # Use its own predictions as the next input
                topv, topi = output[0, :].topk(1)
                prev_word = topi.squeeze().detach()  # detach from history as input

                if prev_word.item() == EOS_token:
                    break

        outputs = torch.cat(outputs, dim=0)  # [max_length, batch_size, output_dictionary_size]

        return outputs, hidden

    def init_hidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

if __name__ == '__main__':
    """This is the stuff that should be done through the notebook, rest can be called I guess
    """
    SOS_token = "SOS"
    EOS_token = "EOS"

    data_dir = './data/'
    complete_dir = os.path.join(data_dir,'composers/notewise/piano_solo/note_range38/sample_freq12/bach')
    train_path = os.path.join(data_dir,'train/')
    test_path = os.path.join(data_dir,'test/')

    val_data_amount = 0.3 #Rest will be training data

    print('The complete directory from where the files are loaded is ', complete_dir)

    files = load_files(complete_dir)
    print('Loaded %d files' % len(files))

    t_seqs_input, t_seqs_target, v_seqs_input, v_seqs_target, notes_indices, indices_notes = create_dataset(files, val_data_amount)

    skip_training = False
    device = torch.device("cpu")

    SOS_token = notes_indices[SOS_token]
    EOS_token = notes_indices[EOS_token]

    # Create encoder and decoder
    hidden_size = 256
    encoder = Encoder(len(notes_indices), hidden_size).to(device)
    decoder = Decoder(hidden_size, len(notes_indices)).to(device)

    teacher_forcing_ratio = 0.5

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)
    criterion = nn.NLLLoss(reduction='sum')

    n_epochs = 8
    t_seqs_input = torch.tensor(t_seqs_input)
    t_seqs_target = torch.tensor(t_seqs_target)

    print("The shape is ",t_seqs_input.shape)
    #trainloader = torch.utils.data.DataLoader(t_seqs, batch_size=1, shuffle=True)

    for epoch in range(n_epochs):
        running_loss = 0.0
        print_every = 100  # pairs
        #for i, (input_seq, target_seq) in enumerate(trainloader):
        #input_seq = t_seqs_input
        #target_seq = t_seqs_target

        for i in range(len(t_seqs_input)):
            #We process one sequence at a time
            #input_seq, target_seq = input_seq[0], target_seq[0]
            #print(i)
            #print(input_seq[1,:])
            #print(input_seq[2,:])

            input_seq = t_seqs_input[i,:].view(-1,1)
            target_seq = t_seqs_input[i,:].view(-1,1)

            input_seq, target_seq = input_seq.to(device), target_seq.to(device)
            

            encoder_hidden = encoder.init_hidden()
            encoder_optimizer.zero_grad()
            decoder_optimizer.zero_grad()

            #input_length = input_seq.size(0)
            target_length = target_seq.size(0)

            # YOUR CODE HERE
            encoder_outputs, hidden = encoder.forward(input_seq, encoder_hidden)
            
    
            if random.random() < teacher_forcing_ratio:
                teacher_forcing=True
                target_val = target_seq
            else:
                teacher_forcing = False
                target_val = None
            
            #print("Teacher forcing: " + str(teacher_forcing) + " Target seq size: " + str(target_seq.size()) + " Input seq size: " + str(input_seq.size()) )
            
            decoder_outputs, hidden = decoder.forward(hidden, target_seq, teacher_forcing)
            
        
            # Compute the loss
            # In case of no teacher forcing, the output sequence can be shorter than the target sequence
            # We need to take care of that
            output_length, _, output_dictionary_size = decoder_outputs.size()
            #print("Teacher forcing: "+str(teacher_forcing)+" Output length: " + str(output_length) + " Target length: " + str(target_length))
            assert (output_length == target_length) or not teacher_forcing, \
                "In case of teacher forcing, output_length ({}) should be equal to target_length ({}).".format(
                output_length, target_length)
            
            loss = criterion(decoder_outputs.view(output_length, output_dictionary_size),
                            target_seq[:output_length].view(output_length))

            loss.backward()

            encoder_optimizer.step()
            decoder_optimizer.step()

            # print statistics
            running_loss += loss.item() / target_length
            if (i % print_every) == (print_every-1):
                print('[%d, %5d] loss: %.4f' % (epoch+1, i+1, running_loss/print_every))
                
                running_loss = 0.0
            
            if skip_training:
                break
        if skip_training:
            break

        print('Finished Training')